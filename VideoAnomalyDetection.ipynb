{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DotunOluyade/AdvanceJava/blob/master/VideoAnomalyDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqG2VmTbSxG7"
      },
      "source": [
        "\n",
        "# Real-Time Crime Prevention using Video Anomaly Detection\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparative Analysis of Pretrained Video Classification models in detecting real-time shooting crime for peace and safety"
      ],
      "metadata": {
        "id": "014sqOa1E1R_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAZ2GqfATJYM"
      },
      "source": [
        "## Data Collection\n",
        "\n",
        "A sub-set of UCF-Crime dataset is used for this research work. It contains 128 hours of video, comprising 1900 long untrimmed real world surveillance videos, with 13 realistic anomalies as well as normal activities (Sultani et al., *2018*).\n",
        "\n",
        "This research uses the shooting dataset only to fine-tune selected video classification pretrained models, compares and evaluate their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specify Dataset Location\n",
        "\n",
        "Import package dependencies and define variables for datasets and pretrained model.\n",
        "\n",
        "Video Swim a pure transformer based video modeling algorithm with its pretrained model is used for feature extraction and fine-tuned with the shooting dataset for classifying videos as shooting or non shooting videos (Liu et al.,2022).\n",
        "\n"
      ],
      "metadata": {
        "id": "uparbOrXN11d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle as sk_shuffle\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import glob\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "normal_dir = '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Training_Normal_Videos_Anomaly'\n",
        "anomaly_dir = '/content/drive/MyDrive/VideoAnomalyDetection/data/UCF-Crime/Shooting'\n",
        "\n",
        "saved_model_path = '/content/drive/MyDrive/VideoAnomalyDetection/pretrained/Video Swin Transformer/TFVideoSwinB_K600_IN22K_P244_W877_32x224'"
      ],
      "metadata": {
        "id": "seFZ6F6PTF71"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility function for Directory Listing of Video Files"
      ],
      "metadata": {
        "id": "GhAk8_f67AxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def list_video_files(directory, extension='*.mp4'):\n",
        "    \"\"\"\n",
        "    List all video files in the given directory with the specified extension.\n",
        "\n",
        "    Parameters:\n",
        "    - directory: Path to the directory to search for video files.\n",
        "    - extension: Extension of the video files to search for (default is '*.mp4').\n",
        "\n",
        "    Returns:\n",
        "    - A list of paths to the video files found.\n",
        "    \"\"\"\n",
        "    # Create a full path pattern by joining directory and extension\n",
        "    pattern = os.path.join(directory, extension)\n",
        "    # Use glob to find all files matching the pattern\n",
        "    video_files = glob.glob(pattern)\n",
        "    return video_files"
      ],
      "metadata": {
        "id": "nx2kYJF-7s_x"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility function to shuffle features and labels list Together"
      ],
      "metadata": {
        "id": "Ld_tFDxMT2W_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_together(list1, list2):\n",
        "    \"\"\"\n",
        "    Shuffle two lists in unison.\n",
        "\n",
        "    Parameters:\n",
        "    - list1: The first list to shuffle.\n",
        "    - list2: The second list to shuffle, must be the same length as list1.\n",
        "\n",
        "    Returns:\n",
        "    - The shuffled list1 and list2.\n",
        "    \"\"\"\n",
        "    if len(list1) != len(list2):\n",
        "        raise ValueError(\"The lists to be shuffled must be the same length.\")\n",
        "\n",
        "    # sklearn's shuffle function to shuffle both lists in unison\n",
        "    list1_shuffled, list2_shuffled = shuffle(list1, list2)\n",
        "    return list1_shuffled, list2_shuffled\n"
      ],
      "metadata": {
        "id": "-MnJMvvu7uZI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Resize, normalize and split videos into frames. Batch frames in 32 segments.\n",
        "\n",
        "Use data generators to prevent loading large datset into memory at once, to better utilize memory for resource constrained environement.\n",
        "\n",
        "Generator loads and preprocess 32 frames at each interval."
      ],
      "metadata": {
        "id": "3Pm7z6-3UOec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_segment_video(video_path, resize_shape=(224, 224), segment_length=32):\n",
        "    \"\"\"\n",
        "    Preprocess raw videos by resizing, color space conversion, normalization\n",
        "\n",
        "    Parameters:\n",
        "    - video_path: absolute path to video file\n",
        "    - resize_shape: shape (224,224) to resize video to\n",
        "    - segment_length: value to segment frames of video to\n",
        "\n",
        "    Returns:\n",
        "    - a batch of 32 preprocessed frames of shape (32, 224, 224, 3), using generators\n",
        "    \"\"\"\n",
        "    def video_generator():\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        frames = []\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break  # Exit loop if video ends\n",
        "            frame = cv2.resize(frame, resize_shape)\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) / 255.0\n",
        "            frames.append(frame)\n",
        "\n",
        "            if len(frames) == segment_length:\n",
        "                yield np.array(frames)\n",
        "                frames = []\n",
        "\n",
        "        # Handle the last segment if it has fewer frames than segment_length\n",
        "        if frames:  # Check if there are remaining frames\n",
        "            # Pad the last segment to have 'segment_length' frames\n",
        "            padding = [np.zeros_like(frames[0]) for _ in range(segment_length - len(frames))]\n",
        "            frames.extend(padding)\n",
        "            yield np.array(frames)  # Yield the padded last segment\n",
        "        cap.release()\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        video_generator,\n",
        "        output_signature=tf.TensorSpec(shape=(segment_length, *resize_shape, 3), dtype=tf.float32)\n",
        "    )\n"
      ],
      "metadata": {
        "id": "77GImlkFzaPW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Dataset into Training & Validation Sets"
      ],
      "metadata": {
        "id": "BIjfSbVTWdsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset_from_videos(video_paths, labels, resize_shape=(224, 224), segment_length=32):\n",
        "    \"\"\"\n",
        "    Creates a TensorFlow dataset of video segments with corresponding labels.\n",
        "\n",
        "    Args:\n",
        "        video_paths (list of str): Paths to video files.\n",
        "        labels (list of int): Labels for each video file.\n",
        "        resize_shape (tuple): The target shape for resizing frames.\n",
        "        segment_length (int): Number of frames per video segment.\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: A dataset of video segments and labels.\n",
        "    \"\"\"\n",
        "    def generator():\n",
        "        for video_path, label in zip(video_paths, labels):\n",
        "            # Use preprocess_and_segment_video to yield segments\n",
        "            for segment in preprocess_and_segment_video(video_path, resize_shape, segment_length):\n",
        "                yield segment.numpy(), label\n",
        "\n",
        "    return tf.data.Dataset.from_generator(\n",
        "        generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(segment_length, *resize_shape, 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(), dtype=tf.int32)\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def create_train_val_datasets(anomaly_dir, normal_dir, train_size=0.8, shuffle_buffer_size=100):\n",
        "    \"\"\"\n",
        "    Create training and validation set\n",
        "\n",
        "    Args:\n",
        "        anomaly_dir: Path to anomalous videos\n",
        "        normal_dir: Path to normal videos\n",
        "        train_size: splitting size for training\n",
        "        shuffle_buffer_size: shuffle size\n",
        "\n",
        "    Returns:\n",
        "        train_dataset, val_dataset\n",
        "    \"\"\"\n",
        "    # list all video files in a directory and their labels\n",
        "    anomaly_videos, normal_videos = list_video_files(anomaly_dir), list_video_files(normal_dir)\n",
        "    video_paths = anomaly_videos + normal_videos\n",
        "    labels = [1] * len(anomaly_videos) + [0] * len(normal_videos)\n",
        "\n",
        "    # Shuffle video_paths and labels in unison\n",
        "    video_paths, labels = shuffle_together(video_paths, labels)\n",
        "\n",
        "    # Create a combined dataset\n",
        "    dataset = create_dataset_from_videos(video_paths, labels)\n",
        "\n",
        "    # Calculate dataset sizes\n",
        "    total_size = len(video_paths)\n",
        "    train_size = int(total_size * train_size)\n",
        "\n",
        "    # Split the dataset\n",
        "    train_dataset = dataset.take(train_size)\n",
        "    val_dataset = dataset.skip(train_size)\n",
        "\n",
        "    return train_dataset, val_dataset\n"
      ],
      "metadata": {
        "id": "4p6erhh4zIII"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tune A Pretrained Video Swin Transformer Model for Video Classification"
      ],
      "metadata": {
        "id": "v4B-y1mRl1Pm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Yzq9StfOfWfv"
      },
      "outputs": [],
      "source": [
        "def build_finetune_model():\n",
        "    \"\"\"\n",
        "    Fine tune pretrained model\n",
        "\n",
        "    Returns:\n",
        "        model (Fine-tuned for shooting classification)\n",
        "    \"\"\"\n",
        "   # import pretrained model, i.e.\n",
        "    video_swin = keras.models.load_model(saved_model_path, compile= False)\n",
        "    video_swin.trainable = False\n",
        "\n",
        "    # downstream model\n",
        "    model = keras.Sequential([\n",
        "        video_swin,\n",
        "        # video_swin outputs a 1D tensor of shape (600,)\n",
        "        layers.Dense(1024, activation='relu'),  # Additional Dense layer\n",
        "        layers.Dropout(0.5),  # Dropout layer to reduce overfitting\n",
        "        layers.Dense(1, activation='sigmoid')  # Final Dense layer for binary classification\n",
        "    ])\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the Fine-Tuned Model for Shooting Classification"
      ],
      "metadata": {
        "id": "z7RHFSSomcmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training and validation data\n",
        "train_dataset, val_dataset = create_train_val_datasets(anomaly_dir, normal_dir, train_size=0.8, shuffle_buffer_size=100)\n",
        "\n",
        "# Batch for memory efficiency\n",
        "batch_size = 1\n",
        "train_dataset = train_dataset.batch(batch_size)\n",
        "val_dataset = val_dataset.batch(batch_size)\n",
        "\n",
        "# Asynchronously fetch batches while model is training\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Only use cache() if memory limits won't be exceeded\n",
        "#train_dataset = train_dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
        "#val_dataset = val_dataset.cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "finetune_model = build_finetune_model()\n",
        "# Compile model with binary entropy loss, adam optimizer and metrics\n",
        "finetune_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[\n",
        "        'accuracy',\n",
        "        tf.keras.metrics.Precision(name='precision'),\n",
        "        tf.keras.metrics.Recall(name='recall'),\n",
        "        tf.keras.metrics.AUC(name='auc')\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = finetune_model.fit(train_dataset, epochs=10,validation_data=val_dataset)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUKmgvFm43KK",
        "outputId": "35be189d-1a19-47f4-8f95-15839a4f3d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "    664/Unknown - 335s 452ms/step - loss: 0.0010 - accuracy: 1.0000 - precision: 0.0000e+00 - recall: 0.0000e+00 - auc: 0.0000e+00"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Final Model Performance"
      ],
      "metadata": {
        "id": "ced1WK7dnwdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot accuracy\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Accuracy over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Loss over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot precision, recall, and AUC\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Precision\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(history.history['precision'], label='Train Precision')\n",
        "plt.plot(history.history['val_precision'], label='Validation Precision')\n",
        "plt.title('Precision over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend()\n",
        "\n",
        "# Recall\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(history.history['recall'], label='Train Recall')\n",
        "plt.plot(history.history['val_recall'], label='Validation Recall')\n",
        "plt.title('Recall over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Recall')\n",
        "plt.legend()\n",
        "\n",
        "# AUC\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(history.history['auc'], label='Train AUC')\n",
        "plt.plot(history.history['val_auc'], label='Validation AUC')\n",
        "plt.title('AUC over epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('AUC')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Evaluate the model\n",
        "#test_loss, test_accuracy, test_precision, test_recall, test_auc = finetune_model.evaluate(test_dataset)\n",
        "#print(f\"Test Metrics:\\n Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, AUC: {test_auc}\")"
      ],
      "metadata": {
        "id": "7VI6xmkh497Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#References:\n",
        "\n",
        "*   Sultani, W., Chen, C. and Shah, M., 2018. Real-world anomaly detection in surveillance videos. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Available at: https://www.crcv.ucf.edu/research/real-world-anomaly-detection-in-surveillance-videos/ 20/03/2024.\n",
        "*   Liu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J., Cao, Y., Zhang, Z., Dong, L. and Wei, F., 2022. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. Available at: https://github.com/innat/VideoSwin 20/03/2024\n"
      ],
      "metadata": {
        "id": "Aq1hW67sMce9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ydJFOP4CMdhA"
      },
      "execution_count": 14,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1gynqv509-foIt7uHod2zNMODzUkFN7MN",
      "authorship_tag": "ABX9TyOmB4G6Ua5LkZliy9+2uXfp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}